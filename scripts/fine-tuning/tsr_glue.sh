python tsr_glue.py \
    --model_name_or_path roberta-base \
    --task_name mrpc \
    --enable_tsr \
    --lora_all_modules \
    --max_length 512 \
    --seed 1234 \
    --lora_r 256 \
    --tsr_scale 2 \
    --proj_type full \
    --lora_r_emb 256 \
    --tsr_scale_emb 2 \
    -proj_type_emb full \
    --per_device_train_batch_size 16 \
    --update_proj_gap 200 \
    --update_proj_gap_emb 200 \
    -learning_rate 3e-5 \
    --num_train_epochs 30 \
    --output_dir results/ft/roberta_base/mrpc